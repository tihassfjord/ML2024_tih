{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 \n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1a) Loss Function in Linear Regression\n",
    "\n",
    "The most commonly used loss function for linear regression is the **Mean Squared Error (MSE)**. This loss function measures the average of the squared differences between the predicted values $ \\hat{y} $ and the actual values $ y $.\n",
    "\n",
    "The mathematical formula for the loss function $ L_{reg}(\\hat{y}, y) $ is given by:\n",
    "\n",
    "$ L_{reg}(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "where:\n",
    "- $ N $ is the number of data points,\n",
    "- $ y_i $ is the actual value for the $i$-th data point,\n",
    "- $ \\hat{y}_i $ is the predicted value for the $i$-th data point.\n",
    "\n",
    "### Derivation of the Gradient\n",
    "To minimize this loss function, we need to compute the gradient of $ L_{reg} $ with respect to the weight parameters $ w_0 $ and $ w_1 $.\n",
    "\n",
    "For a linear model, the prediction is given by:\n",
    "\n",
    "$ \\hat{y} = w_0 + w_1 x $\n",
    "\n",
    "where:\n",
    "- $ w_0 $ is the intercept,\n",
    "- $ w_1 $ is the slope (or weight for the feature $ x $).\n",
    "\n",
    "Using the chain rule, we compute the partial derivatives:\n",
    "\n",
    "1. Gradient with respect to $ w_0 $:\n",
    "$ \\frac{\\partial L_{reg}}{\\partial w_0} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) $\n",
    "\n",
    "2. Gradient with respect to $ w_1 $:\n",
    "$ \\frac{\\partial L_{reg}}{\\partial w_1} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_i $\n",
    "\n",
    "These gradients are used to update the weights during the optimization process, typically with methods like gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Derivation of MSE Gradient (Without Matrix Notation)\n",
    "\n",
    "We begin by defining the Mean Squared Error (MSE) loss function for linear regression:\n",
    "\n",
    "\\[\n",
    "L_{reg}(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( y_i \\) is the actual value,\n",
    "- \\( \\hat{y}_i \\) is the predicted value,\n",
    "- \\( N \\) is the number of data points.\n",
    "\n",
    "The predicted value \\( \\hat{y}_i \\) in linear regression is given by:\n",
    "\n",
    "\\[\n",
    "\\hat{y}_i = w_0 + w_1 x_i\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( w_0 \\) is the intercept,\n",
    "- \\( w_1 \\) is the slope (or weight),\n",
    "- \\( x_i \\) is the input feature for the \\(i\\)-th data point.\n",
    "\n",
    "Our goal is to minimize the loss function by finding the gradients with respect to the parameters \\( w_0 \\) and \\( w_1 \\).\n",
    "\n",
    "### 1. Gradient with respect to \\( w_0 \\)\n",
    "\n",
    "Start by taking the partial derivative of the MSE with respect to \\( w_0 \\):\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L_{reg}}{\\partial w_0} = \\frac{\\partial}{\\partial w_0} \\left( \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (w_0 + w_1 x_i))^2 \\right)\n",
    "\\]\n",
    "\n",
    "We can apply the chain rule here. Letâ€™s denote the error term \\( \\hat{y}_i - y_i = e_i \\), where:\n",
    "\n",
    "\\[\n",
    "e_i = w_0 + w_1 x_i - y_i\n",
    "\\]\n",
    "\n",
    "Now, apply the chain rule:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial}{\\partial w_0} (y_i - \\hat{y}_i)^2 = 2 (y_i - \\hat{y}_i) \\cdot \\frac{\\partial}{\\partial w_0} (-\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Since \\( \\frac{\\partial}{\\partial w_0} \\hat{y}_i = \\frac{\\partial}{\\partial w_0} (w_0 + w_1 x_i) = 1 \\), this simplifies to:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L_{reg}}{\\partial w_0} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)\n",
    "\\]\n",
    "\n",
    "### 2. Gradient with respect to \\( w_1 \\)\n",
    "\n",
    "Similarly, take the partial derivative of the MSE with respect to \\( w_1 \\):\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L_{reg}}{\\partial w_1} = \\frac{\\partial}{\\partial w_1} \\left( \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (w_0 + w_1 x_i))^2 \\right)\n",
    "\\]\n",
    "\n",
    "Again, applying the chain rule:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial}{\\partial w_1} (y_i - \\hat{y}_i)^2 = 2 (y_i - \\hat{y}_i) \\cdot \\frac{\\partial}{\\partial w_1} (-\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Since \\( \\frac{\\partial}{\\partial w_1} \\hat{y}_i = \\frac{\\partial}{\\partial w_1} (w_0 + w_1 x_i) = x_i \\), we get:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L_{reg}}{\\partial w_1} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_i\n",
    "\\]\n",
    "\n",
    "### Final Gradients\n",
    "\n",
    "Thus, the gradients of the MSE loss function with respect to the parameters \\( w_0 \\) and \\( w_1 \\) are:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L_{reg}}{\\partial w_0} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L_{reg}}{\\partial w_1} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_i\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3290196431.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    The Mean Squared Error (MSE) is defined as:\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
